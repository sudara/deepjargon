## [Activation Function](#activation-function)

Normalizes the value of a [neuron](#neuron), squishing it down to a set range such as -1 to 1, or 0 to 1.

Popular activation functions are [sigmoid](#sigmoid) and [ReLU](#relu).

---
1. Watch [Which Activation Function Should I Use?](https://www.youtube.com/watch?v=-7scQpJT7uo) on YouTube.