## [Activation Function](#activation-function)

Normalizes the value of a [neuron](#neuron), squishing it down to a set range such as 0 to 1.

Variants on [ReLU](#relu) are the most popular these days. [Sigmoid](#sigmoid) is the classic educational example.

---
1. Watch [Which Activation Function Should I Use?](https://www.youtube.com/watch?v=-7scQpJT7uo) on YouTube.