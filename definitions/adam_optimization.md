## [Adam Optimization](#adam-optimization)
*Adaptive Moment Estimation*

A way of speeding up [gradient descent](#gradient-descent) that

---
1. Read [Adam Optimizer](https://keras.io/optimizers/#adam) in Keras docs.
2. Watch [Adam Optimization](https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm) on Coursera.