## [Adam Optimization](#adam-optimization)
*Adaptive Moment Estimation*

A way of speeding up [gradient descent](#gradient-descent) that combines [gradient descent with momentum](#momentum) and [RMSprop](#rmsprop).

---
1. Read [Adam Optimizer](https://keras.io/optimizers/#adam) in Keras docs.
2. Watch [Adam Optimization](https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm) on Coursera.
3. Read [Adam Paper](https://arxiv.org/pdf/1412.6980.pdf) on arXiv.