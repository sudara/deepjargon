## [Variance](#variance)

How an algorithm's performance varies from training set to dev set.

If our [training set](#training-set)'s error is low but the [dev set](#dev-set)'s error stays high, then we have high variance and more data is needed. [Regularlization](#regularization) is also a solution, as is changing architectures.

---
1. Watch [Bias/Variance](https://www.coursera.org/learn/deep-neural-network/lecture/ZhclI/bias-variance) on Coursera.
2. Read [Understanding Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html).