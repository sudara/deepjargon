## [Break Symmetry](#break-symmetry)

When weights are [initialized](#initialization) to zero, their values don't differ from each other and they never do anything useful. 

If they are initialized randomly, they each perform differently: symmetry is broken. The [weights](#weight) are no longer identical/similar in value, and therefore have become useful.



---

1. Watch [Random Initialization](https://www.coursera.org/learn/machine-learning/lecture/ND5G5/random-initialization) on Coursera.
2. Read [Why weights should be initialized to random numbers](https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers/40525812) on Stack Overflow.