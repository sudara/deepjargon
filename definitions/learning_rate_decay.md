## [Learning Rate Decay](#learning-rate-decay)

Instead of having a fixed [learning rate](#learning-rate), decreasing the value as [training](#training) progresses in order to expedite the process.

The initial [iterations](#iteration) of [gradient descent](#gradient-descent) will then update the weights more strongly than the later iterations or [epochs](epoch).

This encourages [convergence](#convergence).